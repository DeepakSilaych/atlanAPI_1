[
  {
    "id": 1,
    "name": "Transparent Image Layer Diffusion using Latent Transparency",
    "category": "Category 1",
    "shortdescription": "We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc.",
    "longdescription": "We present LayerDiffuse, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a latent transparency that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.",
    "provider": "Silaych Corporation",
    "input": "input",
    "outpt": "output",
    "stared": 100,
    "gitlink": "#",
    "tags":[
      "image classification",
      "object detection",
      "semantic segmentation"
    ],
    "image": "https://picsum.photos/id/237/200/300",
    "version": "1.0"
  },
  {
    "id": 2,
    "name": "Xception",
    "category": "Category 2",
    "shortdescription": "Xception is a Convolutional networks (ConvNets) in visual recognition developped by Francois Chollet from Google Inc. in 2016 (References: https://arxiv.org/abs/1610.02357). Xception has been trained on the ImageNet ILSVRC-2014.",
    "longdescription": "DescriptionXception is a Convolutional networks (ConvNets) in visual recognition developped by Francois Chollet from Google Inc. in 2016 (References: https://arxiv.org/abs/1610.02357). Xception has been trained on the ImageNet ILSVRC-2014 (http://image-net.org/challenges/LSVRC/2014/index) images database and is able to classify images from 1,000 different categories from spider mite to motor scooter. Xception outperforms Inception V3 on image classification tasks. The model architecture of Xception belongs to the Inception-style models family with so called Inception module which departs from earlier VGG-style networks. Inception modules are similar to classic convolutions (i.e. feature extractors), however they are capable of learning richer representations with less parameters at lower computational cost. The weights were released under the MIT license.",
    "provider": "Provider B",
    "input": "input",
    "outpt": "output",
    "stared": 200,
    "gitlink": "#",
    "tags":[
      "image classification",
      "object detection",
      "semantic segmentation"
    ],
    "image": "https://picsum.photos/id/870/200/300?grayscale&blur=2",
    "version": "1.0"
  
  },
  {
    "id": 3,
    "name": "ResNet-50",
    "category": "Category 3",
    "shortdescription": "ResNet-50 is a Convolutional networks (ConvNets) in visual recognition developped by Kaiming He from Microsoft Research in 2015 (References: https://arxiv.org/abs/1512.03385). ResNet-50 has been trained on the ImageNet ILSVRC-2012.",
    "longdescription": "ResNet-50 is a Convolutional networks (ConvNets) in visual recognition developped by Kaiming He from Microsoft Research in 2015 (References: https://arxiv.org/abs/1512.03385). ResNet-50 has been trained on the ImageNet ILSVRC-2012 (http://image-net.org/challenges/LSVRC/2012/index) images database and is able to classify images from 1,000 different categories from spider mite to motor scooter. ResNet-50 outperforms VGG-16 on image classification tasks. The model architecture of ResNet-50 is based on residual learning, which is a new architecture for ConvNets. The weights were released under the MIT license.",
    "provider": "Provider C",
    "input": "input",
    "outpt": "output",
    "stared": 300,
    "gitlink": "#",
    "tags":[
      "image classification",
      "object detection",
      "semantic segmentation"
    ],
    "image": "https://picsum.photos/id/870/200/300?grayscale&blur=2",
    "version": "1.0"
  },
  {
    "id": 4,
    "name": "Inception V3",
    "category": "Category 4",
    "shortdescription": "Inception V3 is a Convolutional networks (ConvNets) in visual recognition developped by Google Inc. in 2015 (References: https://arxiv.org/abs/1512.00567). Inception V3 has been trained on the ImageNet ILSVRC-2012.",
    "longdescription": "Inception V3 is a Convolutional networks (ConvNets) in visual recognition developped by Google Inc. in 2015 (References: https://arxiv.org/abs/1512.00567). Inception V3 has been trained on the ImageNet ILSVRC-2012 (http://image-net.org/challenges/LSVRC/2012/index) images database and is able to classify images from 1,000 different categories from spider mite to motor scooter. Inception V3 outperforms VGG-16 on image classification tasks. The model architecture of Inception V3 belongs to the Inception-style models family with so called Inception module which departs from earlier VGG-style networks. Inception modules are similar to classic convolutions (i.e. feature extractors), however they are capable of learning richer representations with less parameters at lower computational cost. The weights were released under the MIT license.",
    "provider": "Provider D",
    "input": "input",
    "outpt": "output",
    "stared": 400,
    "gitlink": "#",
    "tags":[
      "image classification",
      "object detection",
      "semantic segmentation"
    ],
    "image": "https://picsum.photos/id/870/200/300?grayscale&blur=2",
    "version": "1.0"
  },
  {
    "id": 5,
    "name": "MobileNetV2",
    "category": "Category 5",
    "shortdescription": "MobileNetV2 is a Convolutional networks (ConvNets) in visual recognition developped by Google Inc. in 2018 (References: https://arxiv.org/abs/1801.04381). MobileNetV2 has been trained on the ImageNet ILSVRC-2012.",
    "longdescription": "MobileNetV2 is a Convolutional networks (ConvNets) in visual recognition developped by Google Inc. in 2018 (References: https://arxiv.org/abs/1801.04381). MobileNetV2 has been trained on the ImageNet ILSVRC-2012 (http://image-net.org/challenges/LSVRC/2012/index) images database and is able to classify images from 1,000 different categories from spider mite to motor scooter. MobileNetV2 outperforms MobileNetV1 on image classification tasks. The model architecture of MobileNetV2 is based on inverted residuals and linear bottlenecks. The weights were released under the MIT license.",
    "provider": "Provider E",
    "input": "input",
    "outpt": "output",
    "stared": 500,
    "gitlink": "#",
    "tags":[
      "image classification",
      "object detection",
      "semantic segmentation"
    ],
    "image": "https://picsum.photos/id/870/200/300?grayscale&blur=2",
    "version": "1.0"
  }
]
